import json
import logging
from typing import Dict, List, Optional, Tuple
from openai import OpenAI
from config.settings import Config

class LLMService:
    """OpenAI LLM APIë¥¼ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config = Config
        self.api_key = self.config.LLM_API_KEY
        self.model = self.config.LLM_MODEL
        self.temperature = self.config.LLM_TEMPERATURE
        self.client = None
        
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key)
    
    def extract_structured_data(self, raw_text: str) -> Dict:
        """
        OCR í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ (Redis ìºì‹± ì ìš©)
        
        Args:
            raw_text: OCRë¡œ ì¶”ì¶œí•œ ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            Dict: êµ¬ì¡°í™”ëœ ì˜ìˆ˜ì¦ ë°ì´í„°
        """
        # Redis ìºì‹± ì ìš©
        try:
            from services.cache_service import redis_cache_manager
            import hashlib
            
            # í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„± (í”„ë¡¬í”„íŠ¸ ê¸°ë°˜)
            prompt = self._build_extraction_prompt(raw_text)
            prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
            
            # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
            cached_result = redis_cache_manager.get_cached_llm_response(prompt_hash)
            if cached_result:
                logging.info(f"ğŸ¯ LLM cache hit - using cached result for {prompt_hash[:12]}...")
                return cached_result
        except Exception as cache_error:
            logging.warning(f"LLM cache check failed: {cache_error}")
        
        if not self.client:
            logging.warning("LLM API key not set. Using mock extraction.")
            mock_result = self._get_mock_structured_data()
            
            # Mock ê²°ê³¼ë„ ìºì‹± (30ë¶„)
            try:
                redis_cache_manager.cache_llm_response(prompt_hash, mock_result, expire_minutes=30)
            except:
                pass
            
            return mock_result
        
        try:
            logging.info(f"ğŸ¤– Processing new text with LLM: {prompt_hash[:12]}...")
            
            logging.info("Sending data extraction request to OpenAI")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=self.temperature
            )
            
            result_text = response.choices[0].message.content
            
            if not result_text:
                raise ValueError("Empty response from LLM")
            
            logging.info(f"Received LLM extraction response: {result_text}")
            extracted_data = json.loads(result_text)
            
            # ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
            validated_data = self._validate_and_clean_extracted_data(extracted_data)
            
            # ì„±ê³µí•œ ê²°ê³¼ë¥¼ Redisì— ìºì‹± (2ì‹œê°„)
            try:
                redis_cache_manager.cache_llm_response(prompt_hash, validated_data, expire_minutes=120)
                logging.info(f"âœ… LLM result cached for {prompt_hash[:12]}...")
            except Exception as cache_error:
                logging.warning(f"LLM result caching failed: {cache_error}")
            
            return validated_data
            
        except Exception as e:
            logging.error(f"LLM data extraction failed: {e}")
            fallback_result = {"error": "LLM data extraction failed"}
            
            # ì‹¤íŒ¨ ì‹œì—ë„ ì§§ì€ ì‹œê°„ ìºì‹± (ì¬ì‹œë„ ë°©ì§€)
            try:
                redis_cache_manager.cache_llm_response(prompt_hash, fallback_result, expire_minutes=10)
            except:
                pass
            
            return fallback_result
    
    def suggest_account_category(self, 
                               ocr_data: str, 
                               amount: int, 
                               usage_date: str, 
                               account_categories: List[Dict],
                               historical_suggestion: Optional[Tuple[str, str]] = None) -> Tuple[str, str]:
        """
        ê³„ì •ê³¼ëª©ê³¼ ì„¤ëª… ì¶”ì²œ
        
        Args:
            ocr_data: OCR ì¶”ì¶œ ë°ì´í„° (ì‚¬ìš©ì²˜)
            amount: ê¸ˆì•¡
            usage_date: ì‚¬ìš©ì¼
            account_categories: ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì •ê³¼ëª© ëª©ë¡
            historical_suggestion: DBì—ì„œ ì°¾ì€ íˆìŠ¤í† ë¦¬ ì œì•ˆ
            
        Returns:
            Tuple[account_category, description]: ì¶”ì²œëœ ê³„ì •ê³¼ëª©ê³¼ ì„¤ëª…
        """
        if not self.client:
            logging.warning("LLM API key not set. Using fallback suggestion.")
            return self._get_fallback_suggestion(ocr_data, historical_suggestion)
        
        try:
            prompt = self._build_categorization_prompt(
                ocr_data, amount, usage_date, account_categories, historical_suggestion
            )
            
            logging.info("Sending categorization request to OpenAI")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=self.temperature
            )
            
            result_text = response.choices[0].message.content
            
            if not result_text:
                raise ValueError("Empty response from LLM")
            
            result = json.loads(result_text)
            account_category = result.get('account_category', 'ì†Œëª¨í’ˆë¹„')
            description = result.get('description', 'ê¸°íƒ€')
            
            logging.info(f"LLM suggested: {account_category} / {description}")
            return account_category, description
            
        except Exception as e:
            logging.error(f"LLM categorization failed: {e}")
            return self._get_fallback_suggestion(ocr_data, historical_suggestion)
    
    def _build_extraction_prompt(self, raw_text: str) -> str:
        """ë°ì´í„° ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""ë‹¤ìŒì€ í•œêµ­ì–´ ì˜ìˆ˜ì¦ì—ì„œ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
ì´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì•„ë˜ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ì›ë³¸ í…ìŠ¤íŠ¸:**
{raw_text}

**ì¶”ì¶œí•  ì •ë³´:**
- merchant_name: ìƒí˜¸ëª…/ì‚¬ìš©ì²˜ (ì˜ˆ: "ìŠ¤íƒ€ë²…ìŠ¤ ê°•ë‚¨ì ")
- transaction_date: ê±°ë˜ì¼ (YYYY-MM-DD í˜•ì‹)
- total_price: ì´ ê¸ˆì•¡ (ì •ìˆ˜, ë‹¨ìœ„: ì›)
- approval_no: ìŠ¹ì¸ë²ˆí˜¸ (ì¹´ë“œ ê²°ì œì‹œ)

**ì‘ë‹µ í˜•ì‹:**
{{
    "merchant_name": "ì¶”ì¶œëœ ìƒí˜¸ëª…",
    "transaction_date": "2025-07-09",
    "total_price": 15000,
    "approval_no": "ìŠ¹ì¸ë²ˆí˜¸ ë˜ëŠ” null"
}}

**ì£¼ì˜ì‚¬í•­:**
- ì •í™•íˆ ì‹ë³„í•  ìˆ˜ ì—†ëŠ” í•„ë“œëŠ” nullë¡œ ì„¤ì •
- ê¸ˆì•¡ì€ ìˆ«ìë§Œ (ì½¤ë§ˆ, ì› ë‹¨ìœ„ ì œê±°)
- ë‚ ì§œëŠ” ë°˜ë“œì‹œ YYYY-MM-DD í˜•ì‹
- JSON í˜•ì‹ë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

    def _build_categorization_prompt(self, 
                                   ocr_data: str, 
                                   amount: int, 
                                   usage_date: str, 
                                   account_categories: List[Dict],
                                   historical_suggestion: Optional[Tuple[str, str]]) -> str:
        """ê³„ì •ê³¼ëª© ë¶„ë¥˜ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ê³„ì •ê³¼ëª© ëª©ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        categories_text = "\\n".join([
            f"- {cat['name']} ({cat['code']}): {cat['description']}" 
            for cat in account_categories
        ])
        
        # íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        history_context = ""
        if historical_suggestion and historical_suggestion[0]:
            history_context = f"""

**ì°¸ê³  ì •ë³´ (ê³¼ê±° ìœ ì‚¬ ê±°ë˜):**
- ê³„ì •ê³¼ëª©: {historical_suggestion[0]}
- ì‚¬ìš© ìš©ë„: {historical_suggestion[1]}
ì´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ë˜, í˜„ì¬ ê±°ë˜ì— ë” ì í•©í•œ ë¶„ë¥˜ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ìš°ì„ í•´ì£¼ì„¸ìš”."""
        
        return f"""ë‹¤ìŒ ì˜ìˆ˜ì¦ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ íšŒê³„ ê³„ì •ê³¼ëª©ê³¼ ì‚¬ìš© ìš©ë„ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ê±°ë˜ ì •ë³´:**
- ì‚¬ìš©ì²˜: {ocr_data}
- ê¸ˆì•¡: {amount:,}ì›
- ì‚¬ìš©ì¼: {usage_date}
{history_context}

**ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì •ê³¼ëª© ë° í•´ë‹¹ í•­ëª©ë“¤:**
{categories_text}

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
    "account_category": "ì¶”ì²œ ê³„ì •ê³¼ëª©ëª…",
    "description": "êµ¬ì²´ì ì¸ ì‚¬ìš© ìš©ë„ (ì˜ˆ: ì•¼ê·¼ ì‹ëŒ€, ìˆ™ë°•ë¹„, ì»¤í”¼)",
    "reasoning": "ì„ íƒ ì´ìœ "
}}

**ì¤‘ìš”í•œ êµ¬ë¶„:**
- account_category: íšŒê³„ìƒ ë¶„ë¥˜ (ì˜ˆ: ë³µë¦¬í›„ìƒë¹„, ì—¬ë¹„êµí†µë¹„)
- description: ì‹¤ì œ ì‚¬ìš© ëª©ì /ìš©ë„ (ì˜ˆ: ì•¼ê·¼ ì‹ëŒ€, ìˆ™ë°•ë¹„, ì»¤í”¼, ì£¼ìœ ë¹„)

**ë¶„ë¥˜ ê°€ì´ë“œë¼ì¸:**
- ì‚¬ìš©ì²˜ì™€ ìœ„ ê³„ì •ê³¼ëª© ì„¤ëª…ì„ ë§¤ì¹­í•˜ì—¬ ì ì ˆí•œ ê³„ì •ê³¼ëª© ì„ íƒ
- descriptionì€ ì‹¤ì œë¡œ ë¬´ì—‡ì— ì‚¬ìš©í–ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
- ê¸ˆì•¡ê³¼ ì‚¬ìš©ì²˜ë¥¼ ì¢…í•© ê³ ë ¤
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

    def _validate_and_clean_extracted_data(self, data: Dict) -> Dict:
        """ì¶”ì¶œëœ ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬"""
        cleaned_data = {}
        
        # merchant_name ì •ë¦¬
        merchant_name = data.get('merchant_name')
        if merchant_name and isinstance(merchant_name, str):
            cleaned_data['merchant_name'] = merchant_name.strip()[:200]  # ê¸¸ì´ ì œí•œ
        else:
            cleaned_data['merchant_name'] = None
        
        # transaction_date ê²€ì¦
        transaction_date = data.get('transaction_date')
        if transaction_date and self._is_valid_date_format(transaction_date):
            cleaned_data['transaction_date'] = transaction_date
        else:
            cleaned_data['transaction_date'] = None
        
        # total_price ì •ë¦¬
        total_price = data.get('total_price')
        if isinstance(total_price, (int, float)) and total_price >= 0:
            cleaned_data['total_price'] = int(total_price)
        else:
            cleaned_data['total_price'] = None
        
        # approval_no ì •ë¦¬
        approval_no = data.get('approval_no')
        if approval_no and isinstance(approval_no, str):
            cleaned_data['approval_no'] = approval_no.strip()[:50]
        else:
            cleaned_data['approval_no'] = None
        
        return cleaned_data
    
    def _is_valid_date_format(self, date_str: str) -> bool:
        """ë‚ ì§œ í˜•ì‹ ê²€ì¦ (YYYY-MM-DD)"""
        try:
            from datetime import datetime
            datetime.strptime(date_str, '%Y-%m-%d')
            return True
        except (ValueError, TypeError):
            return False
    
    def _get_mock_structured_data(self) -> Dict:
        """API í‚¤ê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•  ëª©ì—… ë°ì´í„°"""
        return {
            "merchant_name": "ëª©ì—… ìƒì ",
            "transaction_date": "2025-07-11",
            "total_price": 12345,
            "approval_no": "987654"
        }
    
    def _get_fallback_suggestion(self, 
                               ocr_data: str, 
                               historical_suggestion: Optional[Tuple[str, str]]) -> Tuple[str, str]:
        """LLM ì‚¬ìš© ë¶ˆê°€ì‹œ í´ë°± ì¶”ì²œ"""
        if historical_suggestion:
            return historical_suggestion
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ë¶„ë¥˜ (ê³„ì •ê³¼ëª©, ì‹¤ì œ ì‚¬ìš© ìš©ë„)
        ocr_lower = ocr_data.lower()
        
        if any(keyword in ocr_lower for keyword in ['ìŠ¤íƒ€ë²…ìŠ¤', 'ì¹´í˜', 'ì»¤í”¼']):
            return "ë³µë¦¬í›„ìƒë¹„", "ì»¤í”¼"
        elif any(keyword in ocr_lower for keyword in ['ì£¼ìœ ì†Œ', 'ê¸°ë¦„', 'ì…€í”„']):
            return "ì°¨ëŸ‰ìœ ì§€ë¹„", "ì£¼ìœ ë¹„"
        elif any(keyword in ocr_lower for keyword in ['ë§ˆíŠ¸', 'ìŠˆí¼', 'í¸ì˜ì ']):
            return "ì†Œëª¨í’ˆë¹„", "ìƒí•„í’ˆ êµ¬ë§¤"
        elif any(keyword in ocr_lower for keyword in ['íƒì‹œ', 'ë²„ìŠ¤', 'ì§€í•˜ì² ']):
            return "ì—¬ë¹„êµí†µë¹„", "êµí†µë¹„"
        elif any(keyword in ocr_lower for keyword in ['ìˆ™ë°•', 'í˜¸í…”', 'ëª¨í…”']):
            return "ì—¬ë¹„êµí†µë¹„", "ìˆ™ë°•ë¹„"
        elif any(keyword in ocr_lower for keyword in ['ë³‘ì›', 'ì•½êµ­', 'ì˜ì›']):
            return "ë³µë¦¬í›„ìƒë¹„", "ì˜ë£Œë¹„"
        else:
            return "ì†Œëª¨í’ˆë¹„", "ê¸°íƒ€ êµ¬ë§¤"
    
    def get_model_info(self) -> Dict:
        """í˜„ì¬ ì„¤ì •ëœ LLM ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model": self.model,
            "temperature": self.temperature,
            "api_available": bool(self.client),
            "api_key_configured": bool(self.api_key)
        }

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm_service = LLMService() 